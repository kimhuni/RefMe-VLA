# ğŸš€ evaluation.py
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í›ˆë ¨ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ .jsonl ë°ì´í„°ì…‹ê³¼ ë¹„êµí•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
"""
CUDA_VISIBLE_DEVICES=0 python evaluate/eval_HLP.py \
    --base_model_path /ckpt/Qwen2.5-VL-7B-Instruct \
    --adapter_path /result/ghkim/HLP_qwen_2.5_7b_LoRA_r16_press_the_blue_button_ep60_1109_RAM_test/checkpoint-2000 \
    --dataset_file /data/ghkim/piper_press_the_blue_button_ep60/gpt-5-mini/eval_final/shards/chunk-000.jsonl \
    --output_file /data/ghkim/piper_press_the_blue_button_ep60/eval_qwen_LoRA_RAM_test_2k/shards/chunk_000_evaluation.jsonl \
    --is_qlora True

CUDA_VISIBLE_DEVICES=2 python evaluate/eval_HLP.py \
    --base_model_path /ckpt/Qwen2.5-VL-7B-Instruct \
    --adapter_path /result/ghkim/HLP_qwen_2.5_7b_LoRA_r16_press_the_blue_button_ep60_1109_RAM_test/checkpoint-2000 \
    --dataset_file /data/ghkim/piper_press_the_blue_button_ep60/gpt-5-mini/eval_final/shards/chunk-000.jsonl \
    --output_file /data/ghkim/piper_press_the_blue_button_ep60/eval_qwen_LoRA_RAM_test_2k/shards/chunk_000_evaluation.jsonl \
    --is_qlora False

"""

# ğŸš€ evaluation.py
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í›ˆë ¨ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ .jsonl ë°ì´í„°ì…‹ê³¼ ë¹„êµí•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
import torch
import json
import argparse
import os
from tqdm import tqdm
from PIL import Image
from transformers import (
    Qwen2_5_VLForConditionalGeneration,
    AutoProcessor,
    BitsAndBytesConfig # [ì¶”ê°€] QLoRA ë¡œë”©ì„ ìœ„í•´
)
from peft import PeftModel


# --- 1. dataset_vlm.pyì—ì„œ í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸° ê°€ì ¸ì˜¤ê¸° ---
# (í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ ê²ƒê³¼ *ì •í™•íˆ* ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤)
def make_train_prompt(task: str, prev: str, prev_status: str) -> str:
    """
    Generate a concise training prompt for image analysis in robot manipulation.
    (train_vlm.pyê°€ í›ˆë ¨ ì‹œ ì‚¬ìš©í–ˆë˜ ë°”ë¡œ ê·¸ í•¨ìˆ˜)
    """
    return (
        "You are an image-analysis expert for robot manipulation.\n"
        "INPUT_IMAGES: [SIDE]=global scene view; [WRIST]=close-up wrist camera.\n"
        f"TASK: {task}\n"
        f"PREV_DESC: {prev}\n"
        f"PREV_STATUS: {prev_status}\n"
        "Describe what is visibly happening now (desc_1) and the visible evidence for completion (desc_2).\n"
        "Then decide the status: DONE / NOT_DONE / UNCERTAIN.\n"
        "Output JSON: {\"desc_1\":\"...\",\"desc_2\":\"...\",\"status\":\"...\"}"
    )


def run_evaluation(
    base_model_path: str,
    adapter_path: str,
    dataset_file: str,
    output_file: str,
    load_in_4bit: bool = False, # QLoRA í›ˆë ¨ ì‹œ True
    device: str = "cuda"
):
    """
    .jsonl íŒŒì¼ì„ ì½ì–´ í›ˆë ¨ëœ ëª¨ë¸ì˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³ , ì •ë‹µê³¼ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.
    """

    # QLoRAë¡œ í›ˆë ¨í–ˆë‹¤ë©´, ë² ì´ìŠ¤ ëª¨ë¸ë„ 4ë¹„íŠ¸ë¡œ ë¡œë“œí•´ì•¼ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    bnb_config = None
    if load_in_4bit:
        print("Loading base model in 4-bit (for QLoRA merge)...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )

    print(f"Loading base model from: {base_model_path}")
    # (1) ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        base_model_path,
        quantization_config=bnb_config,  # 4ë¹„íŠ¸ ë¡œë“œ
        device_map=device,
        torch_dtype="auto",
        attn_implementation="eager",
    )

    # (2) í”„ë¡œì„¸ì„œëŠ” ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œì—ì„œ ë¡œë“œ
    processor = AutoProcessor.from_pretrained(base_model_path)
    tokenizer = processor.tokenizer

    print(f"Loading adapter from: {adapter_path}")
    # (3) PEFT ëª¨ë¸ë¡œ ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ ìœ„ì— "ë®ì–´ì”Œì›€"
    model = PeftModel.from_pretrained(
        base_model,
        adapter_path,
        ignore_mismatched_sizes=True
    )

    # (4) â˜…â˜…â˜… ìš”ì²­í•˜ì‹  "ì„ì‹œ ë³‘í•©" (ë©”ëª¨ë¦¬ìƒì—ì„œ ë³‘í•© í›„ PEFT ë˜í¼ ì œê±°) â˜…â˜…â˜…
    print("Merging adapter into base model (in memory)...")
    model = model.merge_and_unload()

    model.eval()  # í‰ê°€ ëª¨ë“œ

    results = []

    print(f"Loading dataset from: {dataset_file}")
    with open(dataset_file, 'r', encoding='utf-8') as f_in:
        # .jsonlì˜ ëª¨ë“  ë¼ì¸ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì½ì–´ì˜´
        lines = f_in.readlines()

    print(f"Running inference on {len(lines)} samples...")
    # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
    for line in tqdm(lines):
        if not line.strip():
            continue

        try:
            data = json.loads(line)

            # --- 3. .jsonlì—ì„œ ì…ë ¥ê°’ ë° ì •ë‹µ ì¶”ì¶œ ---
            # ì…ë ¥ê°’
            side_img_path = data['images']['side']
            wrist_img_path = data['images']['wrist']
            task = data['task']
            prev_desc = data['prev_desc']
            prev_status = data['prev_status']

            # ì •ë‹µ (ë¹„êµìš©)
            ground_truth_output = data['api_output']

            # ì´ë¯¸ì§€ ë¡œë“œ
            images_list = [
                Image.open(side_img_path).convert('RGB'),
                Image.open(wrist_img_path).convert('RGB')
            ]

            # --- 4. í›ˆë ¨ê³¼ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (í•µì‹¬) ---
            user_prompt_text = make_train_prompt(task, prev_desc, prev_status)

            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},  # side
                        {"type": "image"},  # wrist
                        {"type": "text", "text": user_prompt_text}
                    ]
                }
            ]

            # í…œí”Œë¦¿ ì ìš© (evaluate_qwen.py ë°©ì‹)
            prompt_string = tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            # 4-B. [ìˆ˜ì •] processorëŠ” ë³€í™˜ëœ *ë¬¸ìì—´*ê³¼ ì´ë¯¸ì§€ë¥¼ ë°›ì•„ í…ì„œë¡œ ë§Œë“­ë‹ˆë‹¤.
            inputs = processor(
                # [ìˆ˜ì •] ë”•ì…”ë„ˆë¦¬(messages)ê°€ ì•„ë‹Œ ë¬¸ìì—´(prompt_string)ì„ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ ì „ë‹¬
                text=[prompt_string],
                images=images_list,
                padding=True,
                return_tensors="pt"
                # add_generation_promptëŠ” ì´ë¯¸ apply_chat_templateì—ì„œ ì²˜ë¦¬ë¨
            ).to(device)

            # --- 5. ëª¨ë¸ ì¶”ë¡  (model.generate) ---
            with torch.no_grad():  # ì¶”ë¡  ì‹œì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¶ˆí•„ìš”
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=256,  # JSON ì¶œë ¥ì´ë¯€ë¡œ ë„‰ë„‰í•˜ê²Œ
                    do_sample=False  # í‰ê°€ ì‹œì—ëŠ” í•­ìƒ False
                )

            # ì…ë ¥ í† í°ì„ ì œì™¸í•œ ìˆœìˆ˜ ì¶œë ¥ í† í°ë§Œ ë¶„ë¦¬
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]

            # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            output_text = processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]

            # --- 6. ê²°ê³¼ ì €ì¥ (ì´ë¯¸ì§€ ê²½ë¡œ/ë©”íƒ€ í¬í•¨) ---
            uid = data.get("uid", "unknown")
            task = data.get("task", "")
            chunk_id = data.get("chunk_id", "")
            episode_id = data.get("episode_id", "")
            timestamp_ms = data.get("timestamp_ms", None)

            results.append({
                "uid": uid,
                "task": task,
                "chunk_id": chunk_id,
                "episode_id": episode_id,
                "timestamp_ms": timestamp_ms,
                "images": {
                    "side": side_img_path,
                    "wrist": wrist_img_path
                },
                "model_output_raw": output_text,
                "gt_output": ground_truth_output,
                "prompt": user_prompt_text  # ë””ë²„ê¹…ìš© (ì›ì¹˜ ì•Šìœ¼ë©´ ì œê±°í•´ë„ ë¨)
            })

        except Exception as e:
            print(f"Error processing line (uid: {data.get('uid', 'N/A')}): {e}")
            # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë„ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆë¡œ ê¸°ë¡ (ê°€ëŠ¥í•œ í•œ ë©”íƒ€/ì´ë¯¸ì§€ í¬í•¨)
            try:
                uid = data.get("uid", "N/A")
                task = data.get("task", "")
                chunk_id = data.get("chunk_id", "")
                episode_id = data.get("episode_id", "")
                timestamp_ms = data.get("timestamp_ms", None)
                side_img_path = data.get("images", {}).get("side", None)
                wrist_img_path = data.get("images", {}).get("wrist", None)
                gt_out = data.get("api_output", {})
            except Exception:
                uid = "N/A"
                task = chunk_id = episode_id = ""
                timestamp_ms = None
                side_img_path = wrist_img_path = None
                gt_out = {}

            results.append({
                "uid": uid,
                "task": task,
                "chunk_id": chunk_id,
                "episode_id": episode_id,
                "timestamp_ms": timestamp_ms,
                "images": {
                    "side": side_img_path,
                    "wrist": wrist_img_path
                },
                "model_output_raw": f"ERROR: {str(e)}",
                "gt_output": gt_out
            })

    # --- 7. ìµœì¢… ê²°ê³¼ë¥¼ ë³„ë„ jsonl íŒŒì¼ë¡œ ì €ì¥ ---
    print(f"Saving {len(results)} results to {output_file}...")

    output_dir = os.path.dirname(output_file)
    # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
    # (output_dirê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ, ì¦‰ ìƒëŒ€ ê²½ë¡œê°€ ì•„ë‹Œ ê²½ìš°)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    with open(output_file, 'w', encoding='utf-8') as f_out:
        for res in results:
            f_out.write(json.dumps(res) + "\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate a fine-tuned Qwen-VL model by merging adapters in memory")

    # [ìˆ˜ì •ë¨] 2ê°œì˜ ê²½ë¡œë¥¼ ë°›ë„ë¡ ë³€ê²½
    parser.add_argument("--base_model_path", type=str, required=True,
                        help="Path to the ORIGINAL base model directory (e.g., /ckpt/Qwen2.5-VL-7B-Instruct)")
    parser.add_argument("--adapter_path", type=str, required=True,
                        help="Path to the trained LoRA/QLoRA adapter directory (e.g., ./results/.../final-adapter)")

    parser.add_argument("--dataset_file", type=str, required=True,
                        help="Path to the .jsonl dataset file to evaluate")
    parser.add_argument("--output_file", type=str, default="./evaluation_results.jsonl",
                        help="Path to save the evaluation results")

    # [ì¶”ê°€] í›ˆë ¨ ë°©ì‹ì— ë”°ë¼ ì„¤ì •
    parser.add_argument("--is_qlora", type=bool, default=False,
                        help="Set this if you trained with *standard* LoRA (not QLoRA)")

    args = parser.parse_args()

    run_evaluation(
        args.base_model_path,
        args.adapter_path,
        args.dataset_file,
        args.output_file,
        load_in_4bit=args.is_qlora  # í”Œë˜ê·¸ì˜ ë°˜ëŒ€
    )