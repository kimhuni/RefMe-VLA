# ğŸ“‚ inspect_model.py
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor

# âš ï¸ ëª¨ë¸ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”
MODEL_PATH = "/ckpt/Qwen2.5-VL-7B-Instruct"

print(f"Loading model from: {MODEL_PATH}")
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype=torch.bfloat16,
    trust_remote_code=True,
)

print("\n--- Model Architecture (All Modules) ---")
# ëª¨ë“  ëª¨ë“ˆì˜ ì´ë¦„ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
all_modules = {name for name, mod in model.named_modules()}

# LoRAì˜ ì£¼ ëŒ€ìƒì´ ë˜ëŠ” Linear ë ˆì´ì–´ ì´ë¦„ë§Œ í•„í„°ë§í•´ì„œ ë´…ë‹ˆë‹¤.
print("\n--- Candidate Linear Layers for LoRA ---")
count = 0
for name in all_modules:
    # 'q_proj', 'k_proj', 'v_proj', 'o_proj' ë˜ëŠ”
    # 'gate_proj', 'up_proj', 'down_proj' ê°™ì€
    # ì¼ë°˜ì ì¸ LoRA íƒ€ê²Ÿ ë ˆì´ì–´ ì´ë¦„ì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    if any(target in name for target in ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']):
        print(name)
        count += 1

print(f"\nFound {count} candidate layers.")
print("\n---")
print("ì´ì œ ì´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë°˜ë³µë˜ëŠ” í•µì‹¬ ì´ë¦„(ì˜ˆ: 'q_proj', 'k_proj', 'v_proj', 'o_proj')ì„ ì°¾ìœ¼ì„¸ìš”.")
print("ê·¸ ì´ë¦„ë“¤ì„ --target_modules ì¸ìë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.")