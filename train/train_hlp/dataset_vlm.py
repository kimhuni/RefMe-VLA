# ğŸ“¦ dataset_loader.py (V7 - Pre-Caching + All Bug Fixes)
# '100s/it' ë³‘ëª© í˜„ìƒì„ í•´ê²°í•˜ê¸° ìœ„í•´ V6ì˜ 'ì‚¬ì „ ìºì‹±' ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
# V5ì—ì„œ ìˆ˜ì •í•œ ëª¨ë“  ë²„ê·¸(ë§ˆìŠ¤í‚¹, image_grid_thw)ë¥¼ V6 ë¡œì§ì— ì ìš©í•œ ìµœì¢…ë³¸ì…ë‹ˆë‹¤.
# ê²½ê³ : í›ˆë ¨ ì‹œì‘ ì‹œ ëª¨ë“  ë°ì´í„°ë¥¼ RAMì— ìºì‹œí•˜ë¯€ë¡œ, RAM ì‚¬ìš©ëŸ‰ì´ ë§¤ìš° í½ë‹ˆë‹¤.

import json
import os
import glob
import torch
from torch.utils.data import Dataset
from transformers import AutoProcessor, PreTrainedTokenizer
from PIL import Image
import logging
from dataclasses import dataclass
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œ
from typing import List, Dict, Any

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


# (train_vlm.pyì—ì„œ logging.basicConfigë¥¼ í˜¸ì¶œí•´ì•¼ í•¨)


def make_train_prompt(task: str, prev: str, prev_status: str) -> str:
    """
    Generate a concise training prompt for image analysis in robot manipulation.
    """
    return (
        "You are an image-analysis expert for robot manipulation.\n"
        "INPUT_IMAGES: [SIDE]=global scene view; [WRIST]=close-up wrist camera.\n"
        f"TASK: {task}\n"
        f"PREV_DESC: {prev}\n"
        f"PREV_STATUS: {prev_status}\n"
        "Describe what is visibly happening now (desc_1) and the visible evidence for completion (desc_2).\n"
        "Then decide the status: DONE / NOT_DONE / UNCERTAIN.\n"
        "Output JSON: {\"desc_1\":\"...\",\"desc_2\":\"...\",\"status\":\"...\"}"
    )


# ===== 1) Dataset: V6 ì•„í‚¤í…ì²˜ (ì‚¬ì „ ìºì‹±) =====
class VlmDataset(Dataset):
    """
    [V7] __init__ì—ì„œ ëª¨ë“  ìƒ˜í”Œì„ ë¯¸ë¦¬ ì „ì²˜ë¦¬í•˜ì—¬ RAMì— ë³´ê´€í•©ë‹ˆë‹¤.
    __getitem__ì€ ë‹¨ì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ í…ì„œë¥¼ êº¼ë‚´ê¸°ë§Œ í•©ë‹ˆë‹¤ (ë§¤ìš° ë¹ ë¦„).
    """

    def __init__(self, dataset_dir: str, model_name_or_path: str):
        self.processor = None
        self.data = []

        # 1. ìƒ¤ë“œ íŒŒì¼ ê²€ìƒ‰
        shard_pattern = os.path.join(dataset_dir, "shards", "chunk-*.json*")
        shard_files = sorted(glob.glob(shard_pattern))
        if not shard_files:
            raise FileNotFoundError(f"No shards found at {shard_pattern}")

        # 2. .jsonl íŒŒì¼ì˜ ëª¨ë“  ë¼ì¸ì„ ìš°ì„  RAMì— ë¡œë“œ
        for shard_file in shard_files:
            try:
                with open(shard_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            self.data.append(json.loads(line))
            except Exception as e:
                logger.warning(f"Error reading or parsing {shard_file}: {e}")

        logger.info(f"Loaded {len(self.data)} data points. Starting pre-caching...")

        # 3. [í•µì‹¬] ë©”ì¸ í”„ë¡œì„¸ì„œì—ì„œ ì¦‰ì‹œ 'processor'ë¥¼ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(
            model_name_or_path,
            trust_remote_code=True,
            use_fast=True
        )
        self.processor.tokenizer.padding_side = "left"

        # 4. ëª¨ë“  ìƒ˜í”Œì„ ë¯¸ë¦¬ ì „ì²˜ë¦¬í•˜ì—¬ RAM ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        self.processed_samples = []
        # tqdmì„ ì‚¬ìš©í•´ ìºì‹± ì§„í–‰ë¥  í‘œì‹œ
        for i in tqdm(range(len(self.data)), desc="Pre-caching dataset into RAM"):
            try:
                # _process_one_sampleì´ í…ì„œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜
                self.processed_samples.append(
                    self._process_one_sample(self.data[i], i)
                )
            except Exception as e:
                logger.error(f"Failed to process sample {i} ({self.data[i].get('uid', 'N/A')}): {e}")

        logger.info(f"Caching complete. {len(self.processed_samples)} samples loaded into RAM.")
        # ì›ë³¸ ë°ì´í„°ëŠ” ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ
        del self.data

    def __len__(self):
        return len(self.processed_samples)

    def __getitem__(self, i: int):
        # [í•µì‹¬] __getitem__ì€ RAMì— ìºì‹œëœ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¦‰ì‹œ ë°˜í™˜ (ì´ˆê³ ì†)
        return self.processed_samples[i]

    # --- ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ ---
    def _process_one_sample(self, item: dict, idx: int) -> dict:
        """
        [THE REAL FINAL MASKING LOGIC]
        "ê¸¸ì´ ì¶”ì¸¡" (+1) ëŒ€ì‹  "ë‚´ìš© ê²€ìƒ‰"ì„ ì‚¬ìš©í•˜ì—¬ ë§ˆìŠ¤í‚¹ ë²„ê·¸ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
        """

        # --- 1. ì´ë¯¸ì§€ ë¡œë“œ (PIL) ---
        try:
            images_list = [
                Image.open(item['images']['side']).convert('RGB'),
                Image.open(item['images']['wrist']).convert('RGB')
            ]
        except Exception as e:
            logger.error(f"Error loading images for {item.get('uid', idx)}: {e}")
            raise e

        # --- 2. í…ìŠ¤íŠ¸ ìƒì„± ---
        user_prompt_text = make_train_prompt(
            item['task'], item.get('prev_desc', ''), item.get('prev_status', 'NOT_DONE')
        )
        target_text = json.dumps(item['api_output'])  # ì´ê²ƒì´ "ìˆœìˆ˜ JSON"

        # --- 3. ì±„íŒ… í…œí”Œë¦¿ êµ¬ì„± ---
        messages = [
            {"role": "user",
             "content": [{"type": "image"}, {"type": "image"}, {"type": "text", "text": user_prompt_text}]},
            {"role": "assistant", "content": target_text}
        ]

        # --- 4. í† í°í™” (String ë³€í™˜ -> Processor í˜¸ì¶œ) ---
        try:
            prompt_string = self.processor.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
        except Exception as e:
            logger.error(f"Error applying chat template for {item.get('uid', idx)}: {e}")
            raise e

        model_inputs = self.processor(
            text=prompt_string,
            images=images_list,
            return_tensors="pt",
            padding=False
        )

        input_ids = model_inputs['input_ids'].squeeze(0)
        labels = input_ids.clone()
        attention_mask = model_inputs['attention_mask'].squeeze(0)
        pixel_values = model_inputs['pixel_values'].squeeze(0)

        # --- 5. [ìˆ˜ì •ë¨] "ë‚´ìš© ê²€ìƒ‰" ê¸°ë°˜ ë§ˆìŠ¤í‚¹ ---

        # (1) ìš°ë¦¬ê°€ ì˜ˆì¸¡í•´ì•¼ í•  *ìˆœìˆ˜ JSON* í† í°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        target_tokens_ids = self.processor.tokenizer(target_text, add_special_tokens=False).input_ids

        # (2) input_idsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê²€ìƒ‰ìš©)
        full_ids_list = input_ids.tolist()

        # (3) input_ids *ë*ì—ì„œë¶€í„° *ìˆœìˆ˜ JSON* ì‹œí€€ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        start_index = -1
        # (ë‹¨ìˆœí•˜ì§€ë§Œ í™•ì‹¤í•œ ì—­ë°©í–¥ ê²€ìƒ‰)
        for i in range(len(full_ids_list) - len(target_tokens_ids), -1, -1):
            if full_ids_list[i: i + len(target_tokens_ids)] == target_tokens_ids:
                start_index = i
                break  # ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨

        if start_index != -1:
            # (4) JSONì´ ì‹œì‘í•˜ëŠ” ì§€ì (start_index) *ì•*ì„ ëª¨ë‘ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
            labels[:start_index] = -100
        else:
            # (5) [ì¹˜ëª…ì ] ì •ë‹µ JSONì„ input_idsì—ì„œ ì°¾ì§€ ëª»í•¨.
            #     ì´ ìƒ˜í”Œì€ í›ˆë ¨í•˜ë©´ ì•ˆ ë¨. (loss=0, garbage outputì˜ ì›ì¸)
            logger.error(
                f"CRITICAL MASKING ERROR: Target JSON not found in input_ids for {item.get('uid', idx)}. Masking all labels.")
            labels[:] = -100  # ì´ ìƒ˜í”Œ ì „ì²´ë¥¼ ë§ˆìŠ¤í‚¹

        # --- 6. `image_grid_thw` ë³µì› ---
        grid_thw = model_inputs.get("image_grid_thw")
        if grid_thw is not None:
            grid_thw = grid_thw.squeeze(0)

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
            "pixel_values": pixel_values,
            "image_grid_thw": grid_thw,  # [ë³µì›]
        }


# ===== 2) Collator: í…ì„œë¥¼ ë°›ì•„ íŒ¨ë”©ë§Œ ìˆ˜í–‰ (V6ì™€ ê±°ì˜ ë™ì¼) =====
@dataclass
class DataCollatorForVLM:
    """
    VlmDatasetì—ì„œ ì´ë¯¸ RAMì— ìºì‹œëœ í…ì„œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ì•„ íŒ¨ë”©ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    # tokenizer: AutoProcessor
    def __init__(self, tokenizer, processor):
        self.processor = processor
        self.tokenizer = tokenizer

        # â˜… ì•ˆì „ì¥ì¹˜: ì¢Œì¸¡ íŒ¨ë”© ê°•ì œ í™•ì¸
        assert getattr(self.tokenizer, "padding_side", None) == "left", \
            f"tokenizer.padding_side is {self.tokenizer.padding_side}, must be 'left' for Qwen2.5-VL + FA2"

    def _left_pad(self, tensors, pad_value):
        """
        Left-pad a list of 1D torch tensors to the same length.
        """
        max_len = max(t.size(0) for t in tensors)
        out = tensors[0].new_full((len(tensors), max_len), pad_value)
        for i, t in enumerate(tensors):
            out[i, -t.size(0):] = t  # right-align sequence => left padding
        return out

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:

        features = [f for f in features if f]
        if not features:
            logger.warning("Data collator received an empty batch.")
            return {}

        pad_token_id = self.tokenizer.pad_token_id

        if not hasattr(self, "_pad_side_logged"):
            logger.info(f"[DataCollatorForVLM] padding_side={self.tokenizer.padding_side}")
            self._pad_side_logged = True

        # 1. í…ìŠ¤íŠ¸ ê´€ë ¨ í…ì„œ íŒ¨ë”©
        input_ids = self._left_pad([f["input_ids"] for f in features], pad_token_id)
        labels = self._left_pad([f["labels"] for f in features], -100)
        attention_mask = self._left_pad([f["attention_mask"] for f in features], 0)

        # Transformers ë²„ì „ì— ë§ê²Œ ìˆ˜ì •
        pixel_values = torch.cat([f["pixel_values"] for f in features], dim=0)

        batch = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
            "pixel_values": pixel_values,  # ì´ì œ 2D Tensorì…ë‹ˆë‹¤.
        }
        ##################
        # 3. Grid ì •ë³´ ì²˜ë¦¬ ë° ì•ˆì „ì¥ì¹˜ ì¶”ê°€
        if features[0].get("image_grid_thw") is not None:
            concatenated_grid_thw = torch.cat([f["image_grid_thw"] for f in features], dim=0)
            batch["image_grid_thw"] = concatenated_grid_thw
        else:
            batch["image_grid_thw"] = None

        return batch