# ğŸ“¦ dataset_loader.py (V7 - Pre-Caching + All Bug Fixes)
# '100s/it' ë³‘ëª© í˜„ìƒì„ í•´ê²°í•˜ê¸° ìœ„í•´ V6ì˜ 'ì‚¬ì „ ìºì‹±' ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
# V5ì—ì„œ ìˆ˜ì •í•œ ëª¨ë“  ë²„ê·¸(ë§ˆìŠ¤í‚¹, image_grid_thw)ë¥¼ V6 ë¡œì§ì— ì ìš©í•œ ìµœì¢…ë³¸ì…ë‹ˆë‹¤.
# ê²½ê³ : í›ˆë ¨ ì‹œì‘ ì‹œ ëª¨ë“  ë°ì´í„°ë¥¼ RAMì— ìºì‹œí•˜ë¯€ë¡œ, RAM ì‚¬ìš©ëŸ‰ì´ ë§¤ìš° í½ë‹ˆë‹¤.

import json
import os
import glob
import torch
from torch.utils.data import Dataset
from transformers import AutoProcessor
from PIL import Image
import logging
from dataclasses import dataclass
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œ
from typing import List, Dict, Any

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


# (train_vlm.pyì—ì„œ logging.basicConfigë¥¼ í˜¸ì¶œí•´ì•¼ í•¨)


def make_train_prompt(task: str, prev: str, prev_status: str) -> str:
    """
    Generate a concise training prompt for image analysis in robot manipulation.
    """
    return (
        "You are an image-analysis expert for robot manipulation.\n"
        "INPUT_IMAGES: [SIDE]=global scene view; [WRIST]=close-up wrist camera.\n"
        f"TASK: {task}\n"
        f"PREV_DESC: {prev}\n"
        f"PREV_STATUS: {prev_status}\n"
        "Describe what is visibly happening now (desc_1) and the visible evidence for completion (desc_2).\n"
        "Then decide the status: DONE / NOT_DONE / UNCERTAIN.\n"
        "Output JSON: {\"desc_1\":\"...\",\"desc_2\":\"...\",\"status\":\"...\"}"
    )


# ===== 1) Dataset: V6 ì•„í‚¤í…ì²˜ (ì‚¬ì „ ìºì‹±) =====
class VlmDataset(Dataset):
    """
    [V7] __init__ì—ì„œ ëª¨ë“  ìƒ˜í”Œì„ ë¯¸ë¦¬ ì „ì²˜ë¦¬í•˜ì—¬ RAMì— ë³´ê´€í•©ë‹ˆë‹¤.
    __getitem__ì€ ë‹¨ì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ í…ì„œë¥¼ êº¼ë‚´ê¸°ë§Œ í•©ë‹ˆë‹¤ (ë§¤ìš° ë¹ ë¦„).
    """

    def __init__(self, dataset_dir: str, model_name_or_path: str):
        self.processor = None
        self.data = []

        # 1. ìƒ¤ë“œ íŒŒì¼ ê²€ìƒ‰
        shard_pattern = os.path.join(dataset_dir, "shards", "chunk-*.json*")
        shard_files = sorted(glob.glob(shard_pattern))
        if not shard_files:
            raise FileNotFoundError(f"No shards found at {shard_pattern}")

        # 2. .jsonl íŒŒì¼ì˜ ëª¨ë“  ë¼ì¸ì„ ìš°ì„  RAMì— ë¡œë“œ
        for shard_file in shard_files:
            try:
                with open(shard_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            self.data.append(json.loads(line))
            except Exception as e:
                logger.warning(f"Error reading or parsing {shard_file}: {e}")

        logger.info(f"Loaded {len(self.data)} data points. Starting pre-caching...")

        # 3. [í•µì‹¬] ë©”ì¸ í”„ë¡œì„¸ì„œì—ì„œ ì¦‰ì‹œ 'processor'ë¥¼ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(
            model_name_or_path,
            trust_remote_code=True,
            use_fast=True
        )

        # 4. ëª¨ë“  ìƒ˜í”Œì„ ë¯¸ë¦¬ ì „ì²˜ë¦¬í•˜ì—¬ RAM ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        self.processed_samples = []
        # tqdmì„ ì‚¬ìš©í•´ ìºì‹± ì§„í–‰ë¥  í‘œì‹œ
        for i in tqdm(range(len(self.data)), desc="Pre-caching dataset into RAM"):
            try:
                # _process_one_sampleì´ í…ì„œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜
                self.processed_samples.append(
                    self._process_one_sample(self.data[i], i)
                )
            except Exception as e:
                logger.error(f"Failed to process sample {i} ({self.data[i].get('uid', 'N/A')}): {e}")

        logger.info(f"Caching complete. {len(self.processed_samples)} samples loaded into RAM.")
        # ì›ë³¸ ë°ì´í„°ëŠ” ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ
        del self.data

    def __len__(self):
        return len(self.processed_samples)

    def __getitem__(self, i: int):
        # [í•µì‹¬] __getitem__ì€ RAMì— ìºì‹œëœ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¦‰ì‹œ ë°˜í™˜ (ì´ˆê³ ì†)
        return self.processed_samples[i]

    # --- ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ ---
    def _process_one_sample(self, item: dict, idx: int) -> dict:
        """
        [BUG FIXED] V5ì˜ ë²„ê·¸ ìˆ˜ì • ë¡œì§ì„ V6 ì•„í‚¤í…ì²˜ì— ì ìš©í•©ë‹ˆë‹¤.
        """

        # --- 1. ì´ë¯¸ì§€ ë¡œë“œ (PIL) ---
        # (V6) ìºì‹±ì„ ìœ„í•´ ì—¬ê¸°ì„œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ê³  í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        try:
            images_list = [
                Image.open(item['images']['side']).convert('RGB'),
                Image.open(item['images']['wrist']).convert('RGB')
            ]
        except Exception as e:
            logger.error(f"Error loading images for {item.get('uid', idx)}: {e}")
            raise e  # ìºì‹± ì¤‘ë‹¨

        # --- 2. í…ìŠ¤íŠ¸ ìƒì„± ---
        user_prompt_text = make_train_prompt(
            item['task'], item.get('prev_desc', ''), item.get('prev_status', 'NOT_DONE')
        )
        target_text = json.dumps(item['api_output'])

        # --- 3. ì±„íŒ… í…œí”Œë¦¿ êµ¬ì„± ---
        messages = [
            {"role": "user",
             "content": [{"type": "image"}, {"type": "image"}, {"type": "text", "text": user_prompt_text}]},
            {"role": "assistant", "content": target_text}
        ]

        # --- 4. í† í°í™” (String ë³€í™˜ -> Processor í˜¸ì¶œ) ---
        # (AttributeError: 'dict' object has no attribute 'replace' ë²„ê·¸ ìˆ˜ì •)
        try:
            prompt_string = self.processor.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False  # ì´ë¯¸ assistant í„´ í¬í•¨
            )
        except Exception as e:
            logger.error(f"Error applying chat template for {item.get('uid', idx)}: {e}")
            raise e

        model_inputs = self.processor(
            text=prompt_string,  # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ë¬¸ìì—´ ì „ë‹¬
            images=images_list,
            return_tensors="pt",
            padding=False
        )

        # (V6) RAM ìºì‹±ì„ ìœ„í•´ í…ì„œì—ì„œ ë°°ì¹˜ ì°¨ì›(0)ì„ ì œê±°í•©ë‹ˆë‹¤.
        input_ids = model_inputs['input_ids'].squeeze(0)
        labels = input_ids.clone()
        attention_mask = model_inputs['attention_mask'].squeeze(0)
        # (V6) pixel_valuesë„ ìºì‹œí•©ë‹ˆë‹¤. (OOM ìœ„í—˜!)
        pixel_values = model_inputs['pixel_values'].squeeze(0)

        # --- 5. [FINAL MASKING LOGIC] ---
        # (loss=6, `,,` ì¶œë ¥ ë²„ê·¸ ìˆ˜ì •)
        assistant_content_str = "\n" + target_text
        target_tokens = self.processor.tokenizer(
            assistant_content_str, add_special_tokens=False
        ).input_ids

        target_len = len(target_tokens) + 1  # +1 for <|im_end|>
        mask_len = len(labels) - target_len

        if mask_len < 0:
            logger.warning(
                f"Masking error for {item.get('uid', idx)}: Target length ({target_len}) is longer than total input ({len(labels)}). Not masking.")
        else:
            labels[:mask_len] = -100

        # 6. [BUG FIX] `image_grid_thw` "ì˜¬ë°”ë¥´ê²Œ" ì¶”ê°€

        # model_inputsì—ì„œ í…ì„œë¥¼ ê°€ì ¸ì˜´ (V6 ë¡œì§)
        grid_thw = model_inputs.get("image_grid_thw")
        if grid_thw is not None:
            grid_thw = grid_thw.squeeze(0)  # 0-dim ì—ëŸ¬ ë°©ì§€

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
            "pixel_values": pixel_values,  # RAMì— ì´ë¯¸ì§€ í…ì„œ ìºì‹œ
            "image_grid_thw": grid_thw,  # [ì¶”ê°€] (Noneì¼ ìˆ˜ë„ ìˆìŒ)
        }


# ===== 2) Collator: í…ì„œë¥¼ ë°›ì•„ íŒ¨ë”©ë§Œ ìˆ˜í–‰ (V6ì™€ ê±°ì˜ ë™ì¼) =====
@dataclass
class DataCollatorForVLM:
    """
    VlmDatasetì—ì„œ ì´ë¯¸ RAMì— ìºì‹œëœ í…ì„œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ì•„ íŒ¨ë”©ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    tokenizer: AutoProcessor

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:

        features = [f for f in features if f]
        if not features:
            logger.warning("Data collator received an empty batch.")
            return {}

        pad_token_id = self.tokenizer.pad_token_id

        # 1. í…ìŠ¤íŠ¸ ê´€ë ¨ í…ì„œ íŒ¨ë”©
        input_ids = pad_sequence(
            [f["input_ids"] for f in features], batch_first=True, padding_value=pad_token_id
        )
        labels = pad_sequence(
            [f["labels"] for f in features], batch_first=True, padding_value=-100
        )
        attention_mask = pad_sequence(
            [f["attention_mask"] for f in features], batch_first=True, padding_value=0
        )

        # 2. ì´ë¯¸ì§€ í…ì„œ ìŠ¤íƒ
        try:
            pixel_values = torch.stack([f["pixel_values"] for f in features])
        except Exception as e:
            shapes = [f["pixel_values"].shape for f in features]
            logger.error(f"Failed to stack pixel_values. Shapes: {shapes}. Error: {e}")
            # V6ëŠ” pixel_valuesë¥¼ ìºì‹œí•˜ë¯€ë¡œ, ì—¬ê¸°ì„œ í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ì¹˜ëª…ì  ì—ëŸ¬ì„
            raise e

        batch = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
            "pixel_values": pixel_values,  # [V6] pixel_valuesë„ ì „ë‹¬
        }

        if features[0]["image_grid_thw"] is not None:
            try:
                concatenated_grid_thw = torch.cat([f["image_grid_thw"] for f in features], dim=0)
                batch["image_grid_thw"] = concatenated_grid_thw
            except Exception as e:
                shapes = [f["image_grid_thw"].shape for f in features if f["image_grid_thw"] is not None]
                logger.error(f"Failed to concatenate image_grid_thw. Shapes: {shapes}. Error: {e}")
                # ì´ ê²½ìš°, Noneìœ¼ë¡œ ë‘ì–´ ëª¨ë¸ì´ ì²˜ë¦¬í•˜ë„ë¡ í•¨
                batch["image_grid_thw"] = None

        else:
            batch["image_grid_thw"] = None  # ëª…ì‹œì ìœ¼ë¡œ None ì „ë‹¬

        # --- [BUG FIX] `image_grid_thw` ì œê±° ---

        return batch